{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Read CSV Data and Sort, Count and View Shuffles and Execution Plans, Filter Operations #\n",
    "Examples based on *Spark: Definitive Guide: Big Data processing Made Simple*, by Mate Zaharia and Bill Chambers - Chapter 2.  \n",
    "\n",
    "In this example, sample flight data for 2010 to 2015 is processed and the execution plan for a wide transformation (shuffle) is demonstrated.  Simple Sort and Count operations are performed against the data.  **Prerequisite:** The sample data can be downloaded to `./datain/flight-data` with the `data-download.ipynb` notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/12 14:46:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-notebook-2\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        config(\"spark.eventLog.enabled\", \"true\").\\\n",
    "        config(\"spark.eventLog.dir\", \"file:///opt/workspace/events\").\\\n",
    "        getOrCreate()      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all the flight-data CSV files in the sample `../datain/flight-data` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "flightData = spark.read.option(\"inferSchema\", True).option(\"header\", True).csv(\"/opt/workspace/datain/flight-data/*.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Data ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View a small sample of the data-set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=264),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=69),\n",
       " Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=24),\n",
       " Row(DEST_COUNTRY_NAME='Equatorial Guinea', ORIGIN_COUNTRY_NAME='United States', count=1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get summary statistics about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-------------------+------------------+\n",
      "|summary|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|             count|\n",
      "+-------+-----------------+-------------------+------------------+\n",
      "|  count|             1502|               1502|              1502|\n",
      "|   mean|             null|               null|1718.3189081225032|\n",
      "| stddev|             null|               null|22300.368619668898|\n",
      "|    min|      Afghanistan|        Afghanistan|                 1|\n",
      "|    max|         Zimbabwe|           Zimbabwe|            370002|\n",
      "+-------+-----------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 1502 Destination countries and 1502 origin countries listed, each with a count value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing Execution Plans ##\n",
    "The Spark `explain()` method can be used to show the execution strategy that will be chosen by Spark to execute a statement.  \n",
    "  \n",
    "In the example below, the `sort()` action requires all data from all partitions to be compared - this causes a *shuffle* AKA *partition exchange* which is shown in the execution plan as *Exchange rangepartitioning*.  This happens after the previous *FileScan* operation which reads all the data in to be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Sort [count#12 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(count#12 ASC NULLS FIRST, 200)\n",
      "   +- *(1) FileScan csv [DEST_COUNTRY_NAME#10,ORIGIN_COUNTRY_NAME#11,count#12] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/opt/workspace/datain/flight-data/2010-summary.csv, file:/opt/workspace/da..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n"
     ]
    }
   ],
   "source": [
    "flightData.sort(\"count\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the Shuffle Partition Configuration ####\n",
    "By setting the `spark.sql.shuffle.partitions` parameter, we can specify how many partitions to use in the data shuffle operation.  The default is 200 - we probably only need 2 for a 2 node cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Sort [count#12 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(count#12 ASC NULLS FIRST, 2)\n",
      "   +- *(1) FileScan csv [DEST_COUNTRY_NAME#10,ORIGIN_COUNTRY_NAME#11,count#12] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/opt/workspace/datain/flight-data/2010-summary.csv, file:/opt/workspace/da..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")\n",
    "flightData.sort(\"count\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='United States', count=370002),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='United States', count=358354),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='United States', count=352742),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='United States', count=348113),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='United States', count=347452)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Due to Spark \"lazy execution\", our sort finally gets executed now (not at the Explain stage)\n",
    "flightData.sort(\"count\",ascending=False).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of a Narrow Execution followed by Wide  - sum up all the counts ####\n",
    "Count can be performed at each partition (a Narrow operation - no shuffle) then the results combined to a single count (a Wide operation / Shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[], functions=[sum(cast(count#12 as bigint))])\n",
      "+- Exchange SinglePartition\n",
      "   +- *(1) HashAggregate(keys=[], functions=[partial_sum(cast(count#12 as bigint))])\n",
      "      +- *(1) FileScan csv [count#12] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/opt/workspace/datain/flight-data/2010-summary.csv, file:/opt/workspace/da..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<count:int>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "flightData.select(F.sum(\"count\")).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sum(count)=2580915)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "flightData.select(F.sum(\"count\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Rows and Columns ##\n",
    "Filter out all the countries where Destination is not United States and then drop the DEST_COUNTRY_NAME column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-notebook-2-filter-count\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        config(\"spark.eventLog.enabled\", \"true\").\\\n",
    "        config(\"spark.eventLog.dir\", \"file:///opt/workspace/events\").\\\n",
    "        getOrCreate()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "flightData = spark.read.option(\"inferSchema\", True).option(\"header\", True).csv(\"/opt/workspace/datain/flight-data/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Project [ORIGIN_COUNTRY_NAME#239, count#240]\n",
      "+- Filter (DEST_COUNTRY_NAME#238 = United States)\n",
      "   +- Relation[DEST_COUNTRY_NAME#238,ORIGIN_COUNTRY_NAME#239,count#240] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "ORIGIN_COUNTRY_NAME: string, count: int\n",
      "Project [ORIGIN_COUNTRY_NAME#239, count#240]\n",
      "+- Filter (DEST_COUNTRY_NAME#238 = United States)\n",
      "   +- Relation[DEST_COUNTRY_NAME#238,ORIGIN_COUNTRY_NAME#239,count#240] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [ORIGIN_COUNTRY_NAME#239, count#240]\n",
      "+- Filter (isnotnull(DEST_COUNTRY_NAME#238) && (DEST_COUNTRY_NAME#238 = United States))\n",
      "   +- Relation[DEST_COUNTRY_NAME#238,ORIGIN_COUNTRY_NAME#239,count#240] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [ORIGIN_COUNTRY_NAME#239, count#240]\n",
      "+- *(1) Filter (isnotnull(DEST_COUNTRY_NAME#238) && (DEST_COUNTRY_NAME#238 = United States))\n",
      "   +- *(1) FileScan csv [DEST_COUNTRY_NAME#238,ORIGIN_COUNTRY_NAME#239,count#240] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/opt/workspace/datain/flight-data/2010-summary.csv, file:/opt/workspace/da..., PartitionFilters: [], PushedFilters: [IsNotNull(DEST_COUNTRY_NAME), EqualTo(DEST_COUNTRY_NAME,United States)], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n"
     ]
    }
   ],
   "source": [
    "# Use Spark.SQL API syntax to filter - use col Function to reference col-name (needs wrapping in parentheses)\n",
    "# To drop multiple columns, create a list and unpack in the function-call EG drop(*my_list_of_columns)\n",
    "flightDataToUSA = flightData.filter((F.col(\"DEST_COUNTRY_NAME\") == \"United States\")).drop(\"DEST_COUNTRY_NAME\").explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightDataToUSA = flightData.filter((F.col(\"DEST_COUNTRY_NAME\") == \"United States\")).drop(\"DEST_COUNTRY_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "| ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-----+\n",
      "|             Romania|    1|\n",
      "|             Ireland|  264|\n",
      "|               India|   69|\n",
      "|           Singapore|   25|\n",
      "|             Grenada|   54|\n",
      "|    Marshall Islands|   44|\n",
      "|        Sint Maarten|   53|\n",
      "|         Afghanistan|    2|\n",
      "|              Russia|  156|\n",
      "|Federated States ...|   48|\n",
      "|         Netherlands|  570|\n",
      "|             Senegal|   46|\n",
      "|              Angola|   18|\n",
      "|            Anguilla|   20|\n",
      "|             Ecuador|  345|\n",
      "|              Cyprus|    1|\n",
      "|Bosnia and Herzeg...|    1|\n",
      "|            Portugal|  104|\n",
      "|          Costa Rica|  501|\n",
      "|           Guatemala|  333|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightDataToUSA.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
