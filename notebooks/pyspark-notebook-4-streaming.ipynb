{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "wireless-frontier",
   "metadata": {},
   "source": [
    "# 4. Structured Streaming with the Retail Data-Set #\n",
    "Use the by-day retail data-set to simulate a daily feed of data to be read by a Spark Structured Streaming job.   \n",
    "\n",
    "Each file that is read in by the `spark.ReadStream` method uses the option *maxFilesPerTrigger=1* to trigger an update to the streaming dataframe contents.  \n",
    "  \n",
    "A transformation called `purchaseByCustomerPerHour` is mapped to the streaming dataframe with a query transformation to show the top 5 customers and their spend so far. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "awful-cameroon",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-notebook-4\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        config(\"spark.eventLog.enabled\", \"true\").\\\n",
    "        config(\"spark.eventLog.dir\", \"file:///opt/workspace/events\").\\\n",
    "        getOrCreate()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "embedded-senegal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Streaming functions for creating window and defining a column (col) to calculate the window over\n",
    "#from pyspark.sql.functions import window, column, desc, col\n",
    "from pyspark.sql.functions import window,  col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-latino",
   "metadata": {},
   "source": [
    "First, get a sample of the data to be processed in a continuous stream.  \n",
    "+ We have to assume that future data will have the same schema.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cross-phrase",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample = spark.read.option(\"inferSchema\", True).option(\"header\", True).csv(\"/opt/workspace/datain/retail-data/by-day/2010-12-01.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "imposed-mambo",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(InvoiceNo='536365', StockCode='85123A', Description='WHITE HANGING HEART T-LIGHT HOLDER', Quantity=6, InvoiceDate=datetime.datetime(2010, 12, 1, 8, 26), UnitPrice=2.55, CustomerID=17850.0, Country='United Kingdom'),\n",
       " Row(InvoiceNo='536365', StockCode='71053', Description='WHITE METAL LANTERN', Quantity=6, InvoiceDate=datetime.datetime(2010, 12, 1, 8, 26), UnitPrice=3.39, CustomerID=17850.0, Country='United Kingdom'),\n",
       " Row(InvoiceNo='536365', StockCode='84406B', Description='CREAM CUPID HEARTS COAT HANGER', Quantity=8, InvoiceDate=datetime.datetime(2010, 12, 1, 8, 26), UnitPrice=2.75, CustomerID=17850.0, Country='United Kingdom'),\n",
       " Row(InvoiceNo='536365', StockCode='84029G', Description='KNITTED UNION FLAG HOT WATER BOTTLE', Quantity=6, InvoiceDate=datetime.datetime(2010, 12, 1, 8, 26), UnitPrice=3.39, CustomerID=17850.0, Country='United Kingdom'),\n",
       " Row(InvoiceNo='536365', StockCode='84029E', Description='RED WOOLLY HOTTIE WHITE HEART.', Quantity=6, InvoiceDate=datetime.datetime(2010, 12, 1, 8, 26), UnitPrice=3.39, CustomerID=17850.0, Country='United Kingdom')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sample.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-denial",
   "metadata": {},
   "source": [
    "#### Create a Streaming Dataframe ####\n",
    "The Streaming Dataframe is created with a Schema based on the data sample we took earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "laden-joining",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a Schema from the data_sample\n",
    "staticSchema = data_sample.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "irish-monte",
   "metadata": {},
   "source": [
    "Create a streaming dataframe using `maxFilesPerTrigger` to trigger an update to the dataframe on file-by-file basis.  The `header` option caters for (assumes) headers on each file that is loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "legendary-identification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Streaming DataFrame based on a Static Schema from the data_sample\n",
    "streamingDataFrame = spark.readStream.schema(staticSchema).option(\"maxFilesPerTrigger\",1).format(\"csv\").option(\"header\", \"true\")\\\n",
    "       .load(\"/opt/workspace/datain/retail-data/by-day/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "heated-portal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the stream status\n",
    "streamingDataFrame.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-conservative",
   "metadata": {},
   "source": [
    "A streaming transformation `purchaseByCustomerPerHour` is created below; This applies a dataframe transformation which groups the data by *CustomerID* and *InvoiceDate* and sums the \"total_cost\" of *UnitPrice* x *Quantity*.  \n",
    "\n",
    "The `window` function is used to define the size of window over which this summary is provided - in this case a 1-day window of time.  This means we get to see which customers have bought the most *in a single day*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "seven-female",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transformation that sum's customer purchase per hour\n",
    "purchaseByCustomerPerHour = streamingDataFrame.selectExpr(\"CustomerID\", \"(UnitPrice * Quantity) as total_cost\", \"InvoiceDate\") \\\n",
    "                                              .groupBy(col(\"CustomerID\"), window(col(\"InvoiceDate\"), \"1 day\")) \\\n",
    "                                              .sum(\"total_cost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-addition",
   "metadata": {},
   "source": [
    "Create an in-memory structure called `customer_purchases` that  is written to with the output from the streaming transformation. This can be queried using Spark SQL to see the latest result-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "initial-disease",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7fa7872199e8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate updates to an in-memory table after each trigger\n",
    "purchaseByCustomerPerHour.writeStream.format(\"memory\").queryName(\"customer_purchases\").outputMode(\"complete\").start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-teacher",
   "metadata": {},
   "source": [
    "Query `customer_purchases` to see the latest view of which customers have spent the most in a single day.  The results change as new retail data day-files are processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "alternate-sydney",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+\n",
      "|CustomerID|              window|   sum(total_cost)|\n",
      "+----------+--------------------+------------------+\n",
      "|   18102.0|[2010-12-07 00:00...|          25920.37|\n",
      "|      null|[2010-12-10 00:00...|25399.560000000012|\n",
      "|      null|[2010-12-17 00:00...|25371.769999999768|\n",
      "|      null|[2010-12-06 00:00...|23395.099999999904|\n",
      "|      null|[2010-12-03 00:00...| 23021.99999999999|\n",
      "+----------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * FROM customer_purchases\n",
    "ORDER BY `sum(total_cost)` DESC\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "unable-venice",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-witness",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
