{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "extensive-circulation",
   "metadata": {},
   "source": [
    "# 5. K-Means Clustering with the Retail Data-Set #\n",
    "Demonstration of using MMlib to apply the K-Means algorithm to Retail data-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "sonic-earthquake",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Spark Server\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-notebook-5\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        config(\"spark.eventLog.enabled\", \"true\").\\\n",
    "        config(\"spark.eventLog.dir\", \"file:///opt/workspace/events\").\\\n",
    "        getOrCreate() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-artist",
   "metadata": {},
   "source": [
    "#### Load Data ####\n",
    "\n",
    "Use `spark.read` to load some data. https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html  \n",
    "Note on the SQL `col` function: https://stackoverflow.com/questions/40163106/cannot-find-col-function-in-pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "random-outreach",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import date_format fuction and col (\"return a column\") function\n",
    "from pyspark.sql.functions import date_format, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "retired-citizenship",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "retailDataFrame = spark.read.option(\"inferSchema\", True).option(\"header\", True).csv(\"/opt/workspace/datain/retail-data/by-day/*.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lined-canadian",
   "metadata": {},
   "source": [
    "Summary of DataFrame methods: https://spark.apache.org/docs/1.6.1/api/java/org/apache/spark/sql/DataFrame.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "scenic-yugoslavia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill Na's and create a day_of_week column.   coalesce(2) function coalesce's data down to 2 node partitions\n",
    "preppedDataFrame = retailDataFrame.na.fill(0).withColumn(\"day_of_week\", date_format(col(\"InvoiceDate\"), \"EEEE\")).coalesce(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-background",
   "metadata": {},
   "source": [
    "#### Explore the Data ####\n",
    "Very brief look at the data-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "tamil-incident",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(InvoiceNo='580538', StockCode='23084', Description='RABBIT NIGHT LIGHT', Quantity=48, InvoiceDate=datetime.datetime(2011, 12, 5, 8, 38), UnitPrice=1.79, CustomerID=14075.0, Country='United Kingdom', day_of_week='Monday'),\n",
       " Row(InvoiceNo='580538', StockCode='23077', Description='DOUGHNUT LIP GLOSS ', Quantity=20, InvoiceDate=datetime.datetime(2011, 12, 5, 8, 38), UnitPrice=1.25, CustomerID=14075.0, Country='United Kingdom', day_of_week='Monday'),\n",
       " Row(InvoiceNo='580538', StockCode='22906', Description='12 MESSAGE CARDS WITH ENVELOPES', Quantity=24, InvoiceDate=datetime.datetime(2011, 12, 5, 8, 38), UnitPrice=1.65, CustomerID=14075.0, Country='United Kingdom', day_of_week='Monday'),\n",
       " Row(InvoiceNo='580538', StockCode='21914', Description='BLUE HARMONICA IN BOX ', Quantity=24, InvoiceDate=datetime.datetime(2011, 12, 5, 8, 38), UnitPrice=1.25, CustomerID=14075.0, Country='United Kingdom', day_of_week='Monday'),\n",
       " Row(InvoiceNo='580538', StockCode='22467', Description='GUMBALL COAT RACK', Quantity=6, InvoiceDate=datetime.datetime(2011, 12, 5, 8, 38), UnitPrice=2.55, CustomerID=14075.0, Country='United Kingdom', day_of_week='Monday')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preppedDataFrame.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "general-master",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+--------------------+------------------+------------------+------------------+-----------+-----------+\n",
      "|summary|         InvoiceNo|         StockCode|         Description|          Quantity|         UnitPrice|        CustomerID|    Country|day_of_week|\n",
      "+-------+------------------+------------------+--------------------+------------------+------------------+------------------+-----------+-----------+\n",
      "|  count|            541909|            541909|              540455|            541909|            541909|            541909|     541909|     541909|\n",
      "|   mean|  559965.752026781|27623.240210938104|             20713.0|  9.55224954743324|4.6111136260826315|11476.974671024102|       null|       null|\n",
      "| stddev|13428.417280792186|16799.737628427614|                 NaN|218.08115785023375| 96.75985306117823|  6777.90832601304|       null|       null|\n",
      "|    min|            536365|             10002| 4 PURPLE FLOCK D...|            -80995|         -11062.06|               0.0|  Australia|     Friday|\n",
      "|    max|           C581569|                 m|   wrongly sold sets|             80995|           38970.0|           18287.0|Unspecified|  Wednesday|\n",
      "+-------+------------------+------------------+--------------------+------------------+------------------+------------------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Summary stats\n",
    "preppedDataFrame.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "front-hundred",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|Country             |count |\n",
      "+--------------------+------+\n",
      "|United Kingdom      |495478|\n",
      "|Germany             |9495  |\n",
      "|France              |8557  |\n",
      "|EIRE                |8196  |\n",
      "|Spain               |2533  |\n",
      "|Netherlands         |2371  |\n",
      "|Belgium             |2069  |\n",
      "|Switzerland         |2002  |\n",
      "|Portugal            |1519  |\n",
      "|Australia           |1259  |\n",
      "|Norway              |1086  |\n",
      "|Italy               |803   |\n",
      "|Channel Islands     |758   |\n",
      "|Finland             |695   |\n",
      "|Cyprus              |622   |\n",
      "|Sweden              |462   |\n",
      "|Unspecified         |446   |\n",
      "|Austria             |401   |\n",
      "|Denmark             |389   |\n",
      "|Japan               |358   |\n",
      "|Poland              |341   |\n",
      "|Israel              |297   |\n",
      "|USA                 |291   |\n",
      "|Hong Kong           |288   |\n",
      "|Singapore           |229   |\n",
      "|Iceland             |182   |\n",
      "|Canada              |151   |\n",
      "|Greece              |146   |\n",
      "|Malta               |127   |\n",
      "|United Arab Emirates|68    |\n",
      "|European Community  |61    |\n",
      "|RSA                 |58    |\n",
      "|Lebanon             |45    |\n",
      "|Lithuania           |35    |\n",
      "|Brazil              |32    |\n",
      "|Czech Republic      |30    |\n",
      "|Bahrain             |19    |\n",
      "|Saudi Arabia        |10    |\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Distribution of records by country\n",
    "from pyspark.sql.functions import desc\n",
    "preppedDataFrame.groupby('Country').count().sort(desc(\"count\")).show(100,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "canadian-testing",
   "metadata": {},
   "source": [
    "-> The data is heavily skewed to the UK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "silver-record",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4070"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distinct number of product codes \n",
    "preppedDataFrame.select('StockCode').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-audit",
   "metadata": {},
   "source": [
    "-> There are 4070 distinct products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-heart",
   "metadata": {},
   "source": [
    "#### Split Data into Test and Training Sets ####\n",
    "*However*, splitting data into test and training sets is not always necessarily required for un-supervised cluster analysis.  However, we want to analysis the cluster separation (\"silhouette\") and error we get with a new data set based on a training set to determine best number of clusters to choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "accredited-collective",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245903\n",
      "296006\n"
     ]
    }
   ],
   "source": [
    "trainDataFrame = preppedDataFrame.where(\"InvoiceDate < '2011-07-01'\")\n",
    "testDataFrame = preppedDataFrame.where(\"InvoiceDate >= '2011-07-01'\")\n",
    "print(trainDataFrame.count())\n",
    "print(testDataFrame.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "provincial-rebecca",
   "metadata": {},
   "source": [
    "#### Use `StringIndexer` and `OneHotEncoder` to Encode Day-of-Week and create a Feature `VectorAssembler` ####\n",
    "Ref: https://spark.apache.org/docs/latest/ml-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "minus-cable",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer # takes range of string labels and maps to integer values\n",
    "from pyspark.ml.feature import OneHotEncoder # One-hot-encoding for categorical features; encode as binary encoded vector-col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "multiple-insert",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer().setInputCol(\"day_of_week\").setOutputCol(\"day_of_week_index\")\n",
    "encoder = OneHotEncoder().setInputCol(\"day_of_week_index\").setOutputCol(\"day_of_week_encoded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-spouse",
   "metadata": {},
   "source": [
    "Assemble multi-value vector of features including one-hot representation for day-of-week into a vector stored in a single column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "agricultural-pavilion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "vectorAssembler = VectorAssembler().setInputCols([\"UnitPrice\", \"Quantity\", \"day_of_week_encoded\"]).setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-carolina",
   "metadata": {},
   "source": [
    "#### Create a Model Training Pipeline ####\n",
    "  \n",
    "Train a K-Means cluster model with *Day-of-Week* and *Unit Price* and *Quantity* in the feature-set\n",
    "\n",
    "First, fit the transformers to the data-set.  The `StringIndexer` needs to know how many unique values there are to index.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "mental-company",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "transformationPipeline = Pipeline().setStages([indexer, encoder, vectorAssembler])\n",
    "fittedPipeline = transformationPipeline.fit(trainDataFrame)\n",
    "transformedTraining = fittedPipeline.transform(trainDataFrame)\n",
    "transformedTest = fittedPipeline.transform(testDataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-lyric",
   "metadata": {},
   "source": [
    "Note: it woud be possible to also include the model training in the pipeline above.  Instead we next *cache* the pipeline and then experiment with different hyperparameters training a model against the same pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "decreased-fusion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string, day_of_week: string, day_of_week_index: double, day_of_week_encoded: vector, features: vector]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformedTraining.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governmental-acquisition",
   "metadata": {},
   "source": [
    "#### Import KMeans from `pyspark.ml.clustering` and Fit a Model ####\n",
    "https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.clustering.KMeans.html#pyspark.ml.clustering.KMeans.k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "younger-participation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Spark ML Kmeans model\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# Evaluate clustering by computing Silhouette score\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "# setK - the number of clusters to create\n",
    "#kmeans = KMeans().setK(5).setSeed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "separate-vessel",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmModel = kmeans.fit(transformedTraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "broadband-custody",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the cost with KMeans.computeCost() - cost is within-set sum of squared errors (distances from center)\n",
    "#print(kmModel.computeCost(transformedTraining))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defined-confidence",
   "metadata": {},
   "source": [
    "#### Look for Optimal Number of Clusters ####\n",
    "Consider **Cluster Separation** and **Standard Error** to evaluate different numbers of clusters to choose.  \n",
    "  \n",
    "This can take several minutes to run. *Skip this step and just choose K=8 clusters to avoid the analysis phase* by skipping ahead to the *\"Identify Clusters in the Test Data-Set\"* section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-heather",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for optimal K number of clusters from 3 to 20\n",
    "costs = []\n",
    "silhouettes = []\n",
    "for K in range(3,20):\n",
    "    kmModel = KMeans().setK(K).setSeed(42).fit(transformedTraining)    \n",
    "    costs.append(kmModel.computeCost(transformedTraining))\n",
    "    # predict clusters with test data-set and determine level of separatation between clusters (1 is optimal)\n",
    "    predictions = kmModel.transform(transformedTest)\n",
    "    silhouette = evaluator.evaluate(predictions)\n",
    "    #print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "    silhouettes.append(silhouette)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approved-witch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(range(3,20), costs)\n",
    "plt.xlabel(\"number of clusters (\\\"K\\\")\")\n",
    "plt.ylabel(\"Std Error\")\n",
    "plt.title(\"Distance from Centers\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(range(3,20), silhouettes)\n",
    "plt.xlabel(\"number of clusters (\\\"K\\\")\")\n",
    "plt.ylabel(\"Silhouette\")\n",
    "plt.title(\"Separation\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplots_adjust(hspace=1)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excellent-liechtenstein",
   "metadata": {},
   "source": [
    "Based on the above outcome a value of between 8 and 15 clusters would be appropriate.  For the rest of this notebook, choose **8** clusters for maximum cluster separation and lower std error (overall distance from cluster center)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-bracelet",
   "metadata": {},
   "source": [
    "#### Identify Clusters in the Test data-set ####\n",
    "Working with the test data-set, generate preditions from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "driven-trainer",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmModel = KMeans().setK(8).setSeed(42).fit(transformedTraining)    \n",
    "predictions = kmModel.transform(transformedTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "several-constitutional",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.9985060075601593\n"
     ]
    }
   ],
   "source": [
    "# Evaluate clustering by computing Silhouette score\n",
    "evaluator = ClusteringEvaluator()\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-equity",
   "metadata": {},
   "source": [
    "#### Analysis ####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "intense-finland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(InvoiceNo='580538', StockCode='23084', Description='RABBIT NIGHT LIGHT', Quantity=48, InvoiceDate=datetime.datetime(2011, 12, 5, 8, 38), UnitPrice=1.79, CustomerID=14075.0, Country='United Kingdom', day_of_week='Monday', day_of_week_index=2.0, day_of_week_encoded=SparseVector(5, {2: 1.0}), features=SparseVector(7, {0: 1.79, 1: 48.0, 4: 1.0}), prediction=0),\n",
       " Row(InvoiceNo='580538', StockCode='23077', Description='DOUGHNUT LIP GLOSS ', Quantity=20, InvoiceDate=datetime.datetime(2011, 12, 5, 8, 38), UnitPrice=1.25, CustomerID=14075.0, Country='United Kingdom', day_of_week='Monday', day_of_week_index=2.0, day_of_week_encoded=SparseVector(5, {2: 1.0}), features=SparseVector(7, {0: 1.25, 1: 20.0, 4: 1.0}), prediction=0),\n",
       " Row(InvoiceNo='580538', StockCode='22906', Description='12 MESSAGE CARDS WITH ENVELOPES', Quantity=24, InvoiceDate=datetime.datetime(2011, 12, 5, 8, 38), UnitPrice=1.65, CustomerID=14075.0, Country='United Kingdom', day_of_week='Monday', day_of_week_index=2.0, day_of_week_encoded=SparseVector(5, {2: 1.0}), features=SparseVector(7, {0: 1.65, 1: 24.0, 4: 1.0}), prediction=0),\n",
       " Row(InvoiceNo='580538', StockCode='21914', Description='BLUE HARMONICA IN BOX ', Quantity=24, InvoiceDate=datetime.datetime(2011, 12, 5, 8, 38), UnitPrice=1.25, CustomerID=14075.0, Country='United Kingdom', day_of_week='Monday', day_of_week_index=2.0, day_of_week_encoded=SparseVector(5, {2: 1.0}), features=SparseVector(7, {0: 1.25, 1: 24.0, 4: 1.0}), prediction=0),\n",
       " Row(InvoiceNo='580538', StockCode='22467', Description='GUMBALL COAT RACK', Quantity=6, InvoiceDate=datetime.datetime(2011, 12, 5, 8, 38), UnitPrice=2.55, CustomerID=14075.0, Country='United Kingdom', day_of_week='Monday', day_of_week_index=2.0, day_of_week_encoded=SparseVector(5, {2: 1.0}), features=SparseVector(7, {0: 2.55, 1: 6.0, 4: 1.0}), prediction=0)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.take(5)\n",
    "#predictions.select(\"features\").take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developing-combination",
   "metadata": {},
   "source": [
    "Other class-labels not used in the feature-set\n",
    "1. StockCode (same as Description?)  \n",
    "2. CustomerID  \n",
    "3. Country  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cognitive-possible",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct StockCode: 4070\n",
      "Distinct CustomerID: 4373\n",
      "Distinct Country: 38\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Distinct StockCode:\", preppedDataFrame.select('StockCode').distinct().count())\n",
    "print(\"Distinct CustomerID:\", preppedDataFrame.select('CustomerID').distinct().count())\n",
    "print(\"Distinct Country:\", preppedDataFrame.select('Country').distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "institutional-validity",
   "metadata": {},
   "source": [
    "#### Question 1: How many distinct clusters do the Countrys belong to? ####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "hispanic-strategy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----------+\n",
      "|prediction|Country             |n_clusters|\n",
      "+----------+--------------------+----------+\n",
      "|0         |Australia           |1         |\n",
      "|7         |Australia           |1         |\n",
      "|0         |Austria             |1         |\n",
      "|0         |Belgium             |1         |\n",
      "|0         |Canada              |1         |\n",
      "|0         |Channel Islands     |1         |\n",
      "|0         |Cyprus              |1         |\n",
      "|0         |Czech Republic      |1         |\n",
      "|0         |Denmark             |1         |\n",
      "|0         |EIRE                |1         |\n",
      "|7         |EIRE                |1         |\n",
      "|0         |European Community  |1         |\n",
      "|0         |Finland             |1         |\n",
      "|0         |France              |1         |\n",
      "|4         |France              |1         |\n",
      "|7         |France              |1         |\n",
      "|0         |Germany             |1         |\n",
      "|0         |Greece              |1         |\n",
      "|0         |Hong Kong           |1         |\n",
      "|0         |Iceland             |1         |\n",
      "|0         |Israel              |1         |\n",
      "|0         |Italy               |1         |\n",
      "|0         |Japan               |1         |\n",
      "|7         |Japan               |1         |\n",
      "|0         |Malta               |1         |\n",
      "|0         |Netherlands         |1         |\n",
      "|7         |Netherlands         |1         |\n",
      "|0         |Norway              |1         |\n",
      "|0         |Poland              |1         |\n",
      "|0         |Portugal            |1         |\n",
      "|0         |RSA                 |1         |\n",
      "|0         |Singapore           |1         |\n",
      "|4         |Singapore           |1         |\n",
      "|0         |Spain               |1         |\n",
      "|0         |Sweden              |1         |\n",
      "|0         |Switzerland         |1         |\n",
      "|0         |USA                 |1         |\n",
      "|0         |United Arab Emirates|1         |\n",
      "|0         |United Kingdom      |1         |\n",
      "|1         |United Kingdom      |1         |\n",
      "|2         |United Kingdom      |1         |\n",
      "|4         |United Kingdom      |1         |\n",
      "|5         |United Kingdom      |1         |\n",
      "|6         |United Kingdom      |1         |\n",
      "|7         |United Kingdom      |1         |\n",
      "|0         |Unspecified         |1         |\n",
      "+----------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Pandas version\n",
    "#pandas_df = predictions.select(\"StockCode\",\"CustomerID\", \"Country\", \"prediction\").toPandas()\n",
    "#pandas_df.groupby('Country')['prediction'].nunique()\n",
    "\n",
    "#Spark Version\n",
    "import pyspark.sql.functions as f\n",
    "#predictions.groupby('Country').agg(f.expr('count(distinct prediction)').alias('n_clusters')).show(100,False)\n",
    "predictions.groupby('prediction', 'Country').agg(f.expr('count(distinct prediction)').alias('n_clusters')).sort(\"Country\", \"prediction\").show(100,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-technique",
   "metadata": {},
   "source": [
    "-> All the countries except the UK are only in one or two clusters.  If the data-set wasn't so heavily skewed to UK data, this might be interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "proved-induction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|prediction| count|\n",
      "+----------+------+\n",
      "|         1|     1|\n",
      "|         6|     3|\n",
      "|         5|     1|\n",
      "|         4|    22|\n",
      "|         7|    92|\n",
      "|         2|     1|\n",
      "|         0|295886|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question 2 - how many items per cluster\n",
    "predictions.groupby('prediction').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-voluntary",
   "metadata": {},
   "source": [
    "-> only a few very special outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "governmental-sudan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the cluster-ids where count is less than 50, look at those items\n",
    "# To filter on the count, need to rename the count column: https://stackoverflow.com/questions/32119936/dataframe-how-to-groupby-count-then-filter-on-count-in-scala\n",
    "outliers = predictions.groupBy('prediction').count().withColumnRenamed(\"count\", \"n\").filter(\"n <= 50\").sort(\"n\").select(\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "lovely-there",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`prediction == 2`' given input columns: [day_of_week, InvoiceDate, UnitPrice, Country, Quantity, InvoiceNo, CustomerID, features, day_of_week_index, prediction, Description, day_of_week_encoded, StockCode];;\\n'Project ['prediction == 2]\\n+- Project [InvoiceNo#9250, StockCode#9251, Description#9252, Quantity#9274, InvoiceDate#9254, UnitPrice#9275, CustomerID#9276, Country#9257, day_of_week#9285, day_of_week_index#9898, day_of_week_encoded#9909, features#9922, UDF(features#9922) AS prediction#14902]\\n   +- Project [InvoiceNo#9250, StockCode#9251, Description#9252, Quantity#9274, InvoiceDate#9254, UnitPrice#9275, CustomerID#9276, Country#9257, day_of_week#9285, day_of_week_index#9898, day_of_week_encoded#9909, UDF(named_struct(UnitPrice, UnitPrice#9275, Quantity_double_VectorAssembler_61158599aa80, cast(Quantity#9274 as double), day_of_week_encoded, day_of_week_encoded#9909)) AS features#9922]\\n      +- Project [InvoiceNo#9250, StockCode#9251, Description#9252, Quantity#9274, InvoiceDate#9254, UnitPrice#9275, CustomerID#9276, Country#9257, day_of_week#9285, day_of_week_index#9898, if (isnull(cast(day_of_week_index#9898 as double))) null else UDF(cast(day_of_week_index#9898 as double)) AS day_of_week_encoded#9909]\\n         +- Project [InvoiceNo#9250, StockCode#9251, Description#9252, Quantity#9274, InvoiceDate#9254, UnitPrice#9275, CustomerID#9276, Country#9257, day_of_week#9285, UDF(cast(day_of_week#9285 as string)) AS day_of_week_index#9898]\\n            +- Filter (cast(InvoiceDate#9254 as string) >= 2011-07-01)\\n               +- Repartition 2, false\\n                  +- Project [InvoiceNo#9250, StockCode#9251, Description#9252, Quantity#9274, InvoiceDate#9254, UnitPrice#9275, CustomerID#9276, Country#9257, date_format(InvoiceDate#9254, EEEE, Some(Etc/UTC)) AS day_of_week#9285]\\n                     +- Project [InvoiceNo#9250, StockCode#9251, Description#9252, coalesce(Quantity#9253, cast(0.0 as int)) AS Quantity#9274, InvoiceDate#9254, coalesce(nanvl(UnitPrice#9255, cast(null as double)), cast(0.0 as double)) AS UnitPrice#9275, coalesce(nanvl(CustomerID#9256, cast(null as double)), cast(0.0 as double)) AS CustomerID#9276, Country#9257]\\n                        +- Relation[InvoiceNo#9250,StockCode#9251,Description#9252,Quantity#9253,InvoiceDate#9254,UnitPrice#9255,CustomerID#9256,Country#9257] csv\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o4092.select.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`prediction == 2`' given input columns: [day_of_week, InvoiceDate, UnitPrice, Country, Quantity, InvoiceNo, CustomerID, features, day_of_week_index, prediction, Description, day_of_week_encoded, StockCode];;\n'Project ['prediction == 2]\n+- Project [InvoiceNo#9250, StockCode#9251, Description#9252, Quantity#9274, InvoiceDate#9254, UnitPrice#9275, CustomerID#9276, Country#9257, day_of_week#9285, day_of_week_index#9898, day_of_week_encoded#9909, features#9922, UDF(features#9922) AS prediction#14902]\n   +- Project [InvoiceNo#9250, StockCode#9251, Description#9252, Quantity#9274, InvoiceDate#9254, UnitPrice#9275, CustomerID#9276, Country#9257, day_of_week#9285, day_of_week_index#9898, day_of_week_encoded#9909, UDF(named_struct(UnitPrice, UnitPrice#9275, Quantity_double_VectorAssembler_61158599aa80, cast(Quantity#9274 as double), day_of_week_encoded, day_of_week_encoded#9909)) AS features#9922]\n      +- Project [InvoiceNo#9250, StockCode#9251, Description#9252, Quantity#9274, InvoiceDate#9254, UnitPrice#9275, CustomerID#9276, Country#9257, day_of_week#9285, day_of_week_index#9898, if (isnull(cast(day_of_week_index#9898 as double))) null else UDF(cast(day_of_week_index#9898 as double)) AS day_of_week_encoded#9909]\n         +- Project [InvoiceNo#9250, StockCode#9251, Description#9252, Quantity#9274, InvoiceDate#9254, UnitPrice#9275, CustomerID#9276, Country#9257, day_of_week#9285, UDF(cast(day_of_week#9285 as string)) AS day_of_week_index#9898]\n            +- Filter (cast(InvoiceDate#9254 as string) >= 2011-07-01)\n               +- Repartition 2, false\n                  +- Project [InvoiceNo#9250, StockCode#9251, Description#9252, Quantity#9274, InvoiceDate#9254, UnitPrice#9275, CustomerID#9276, Country#9257, date_format(InvoiceDate#9254, EEEE, Some(Etc/UTC)) AS day_of_week#9285]\n                     +- Project [InvoiceNo#9250, StockCode#9251, Description#9252, coalesce(Quantity#9253, cast(0.0 as int)) AS Quantity#9274, InvoiceDate#9254, coalesce(nanvl(UnitPrice#9255, cast(null as double)), cast(0.0 as double)) AS UnitPrice#9275, coalesce(nanvl(CustomerID#9256, cast(null as double)), cast(0.0 as double)) AS CustomerID#9276, Country#9257]\n                        +- Relation[InvoiceNo#9250,StockCode#9251,Description#9252,Quantity#9253,InvoiceDate#9254,UnitPrice#9255,CustomerID#9256,Country#9257] csv\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:279)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3412)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1340)\n\tat sun.reflect.GeneratedMethodAccessor254.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-c690a5afe9b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#for cluster in outliers_list:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#    predictions.where(\"prediction = $cluster\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"prediction == 2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1322\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m         \"\"\"\n\u001b[0;32m-> 1324\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1325\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`prediction == 2`' given input columns: [day_of_week, InvoiceDate, UnitPrice, Country, Quantity, InvoiceNo, CustomerID, features, day_of_week_index, prediction, Description, day_of_week_encoded, StockCode];;\\n'Project ['prediction == 2]\\n+- Project [InvoiceNo#9250, StockCode#9251, Description#9252, Quantity#9274, InvoiceDate#9254, UnitPrice#9275, CustomerID#9276, Country#9257, day_of_week#9285, day_of_week_index#9898, day_of_week_encoded#9909, features#9922, UDF(features#9922) AS prediction#14902]\\n   +- Project [InvoiceNo#9250, StockCode#9251, Description#9252, Quantity#9274, InvoiceDate#9254, UnitPrice#9275, CustomerID#9276, Country#9257, day_of_week#9285, day_of_week_index#9898, day_of_week_encoded#9909, UDF(named_struct(UnitPrice, UnitPrice#9275, Quantity_double_VectorAssembler_61158599aa80, cast(Quantity#9274 as double), day_of_week_encoded, day_of_week_encoded#9909)) AS features#9922]\\n      +- Project [InvoiceNo#9250, StockCode#9251, Description#9252, Quantity#9274, InvoiceDate#9254, UnitPrice#9275, CustomerID#9276, Country#9257, day_of_week#9285, day_of_week_index#9898, if (isnull(cast(day_of_week_index#9898 as double))) null else UDF(cast(day_of_week_index#9898 as double)) AS day_of_week_encoded#9909]\\n         +- Project [InvoiceNo#9250, StockCode#9251, Description#9252, Quantity#9274, InvoiceDate#9254, UnitPrice#9275, CustomerID#9276, Country#9257, day_of_week#9285, UDF(cast(day_of_week#9285 as string)) AS day_of_week_index#9898]\\n            +- Filter (cast(InvoiceDate#9254 as string) >= 2011-07-01)\\n               +- Repartition 2, false\\n                  +- Project [InvoiceNo#9250, StockCode#9251, Description#9252, Quantity#9274, InvoiceDate#9254, UnitPrice#9275, CustomerID#9276, Country#9257, date_format(InvoiceDate#9254, EEEE, Some(Etc/UTC)) AS day_of_week#9285]\\n                     +- Project [InvoiceNo#9250, StockCode#9251, Description#9252, coalesce(Quantity#9253, cast(0.0 as int)) AS Quantity#9274, InvoiceDate#9254, coalesce(nanvl(UnitPrice#9255, cast(null as double)), cast(0.0 as double)) AS UnitPrice#9275, coalesce(nanvl(CustomerID#9256, cast(null as double)), cast(0.0 as double)) AS CustomerID#9276, Country#9257]\\n                        +- Relation[InvoiceNo#9250,StockCode#9251,Description#9252,Quantity#9253,InvoiceDate#9254,UnitPrice#9255,CustomerID#9256,Country#9257] csv\\n\""
     ]
    }
   ],
   "source": [
    "outliers_list = outliers.select(\"prediction\").rdd.flatMap(lambda x: x).collect()\n",
    "#for cluster in outliers_list:\n",
    "#    predictions.where(\"prediction = $cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "junior-banking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string, day_of_week: string, day_of_week_index: double, day_of_week_encoded: vector, features: vector, prediction: int]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-copying",
   "metadata": {},
   "outputs": [],
   "source": [
    "retailDataFrame.where(col(\"StockCode\")==23843).show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consecutive-snapshot",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-negotiation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-biology",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
