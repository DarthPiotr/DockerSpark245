{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "victorian-grass",
   "metadata": {},
   "source": [
    "# 3. Read CSV Data and Basic Data Analysis #\n",
    "Examples based on Chapter 2, \"Spark: Definitive Guide: Big Data processing Made Simple\"\n",
    "\n",
    "In this example, sample flight data for 2010 to 2015 is processed and Simple **MAX**, **Top-N** and **Group-By** operations are performed against the data using Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "certified-verse",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-notebook-3\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        config(\"spark.eventLog.enabled\", \"true\").\\\n",
    "        config(\"spark.eventLog.dir\", \"file:///opt/workspace/events\").\\\n",
    "        getOrCreate()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "simplified-omaha",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read flight data in from CSV\n",
    "flightData = spark.read.option(\"inferSchema\", True).option(\"header\", True).csv(\"/opt/workspace/datain/flight-data/*.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-collins",
   "metadata": {},
   "source": [
    "## Spark SQL Views ##\n",
    "A temporary table-view can be constructed on a Spark data-frame with the `createOrReplaceTempView()` method.  \n",
    "\n",
    "This allows the data-frame data to be analysed using ANSI SQL via the temporary table construct, which is an in-memory non-persistant view of the data built on the Spark data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "speaking-world",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData.createOrReplaceTempView(\"flight_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-rebate",
   "metadata": {},
   "source": [
    "#### Group-By Example ####\n",
    "Query the data-frame and count the number of flights grouped by destination country.  \n",
    "Using Spark SQL to query the temporary table `flight_data` creates another Spark data-frame (in this case, \"`results`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "precise-assessment",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, count(1)\n",
    "FROM flight_data\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "binary-upper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#10], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(DEST_COUNTRY_NAME#10, 200)\n",
      "   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#10], functions=[partial_count(1)])\n",
      "      +- *(1) FileScan csv [DEST_COUNTRY_NAME#10] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/opt/workspace/datain/flight-data/2015-summary.csv, file:/opt/workspace/da..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n"
     ]
    }
   ],
   "source": [
    "# View the execution plan for running this query.  \n",
    "results.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-ocean",
   "metadata": {},
   "source": [
    "#### Max-Value Example ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "parliamentary-three",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(count)=370002)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get max flight destination from flights using Spark SQL\n",
    "spark.sql(\"SELECT max(count) from flight_data\").take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cubic-partition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(count)=370002)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pyspark / Python equiv\n",
    "from pyspark.sql.functions import max\n",
    "flightData.select(max(\"count\")).take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subsequent-concentrate",
   "metadata": {},
   "source": [
    "#### Top-N Example ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "civil-gender",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|          2348280|\n",
      "|           Canada|            49052|\n",
      "|           Mexico|            38075|\n",
      "|   United Kingdom|            10946|\n",
      "|            Japan|             9205|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find top 5 destinations\n",
    "top_5_dest = spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, sum(count) as destination_total\n",
    "FROM flight_data\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "ORDER BY sum(count) DESC\n",
    "LIMIT 5\n",
    "\"\"\")\n",
    "top_5_dest.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "motivated-religious",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|          2348280|\n",
      "|           Canada|            49052|\n",
      "|           Mexico|            38075|\n",
      "|   United Kingdom|            10946|\n",
      "|            Japan|             9205|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find top 5 destinations - DataFrame syntax\n",
    "from pyspark.sql.functions import desc\n",
    "flightData.groupBy(\"DEST_COUNTRY_NAME\").sum(\"count\")\\\n",
    "    .withColumnRenamed(\"sum(count)\", \"destination_total\").sort(desc(\"destination_total\"))\\\n",
    "    .limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-willow",
   "metadata": {},
   "source": [
    "#### Show Execution Path ####\n",
    "Aggregation happens in two parts - in the partial_sum and sum calls - this is because summing a list of numbers is commutative* and Spark can perform the sum partition-by-partition.\n",
    "\n",
    "\\* commutative - \" condition that a group of quantities connected by operators gives the same result whatever the order of the quantities involved, e.g. a × b = b × a.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "comfortable-medication",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "TakeOrderedAndProject(limit=5, orderBy=[aggOrder#74L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#10,destination_total#72L])\n",
      "+- *(2) HashAggregate(keys=[DEST_COUNTRY_NAME#10], functions=[sum(cast(count#12 as bigint))])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#10, 200)\n",
      "      +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#10], functions=[partial_sum(cast(count#12 as bigint))])\n",
      "         +- *(1) FileScan csv [DEST_COUNTRY_NAME#10,count#12] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/opt/workspace/datain/flight-data/2015-summary.csv, file:/opt/workspace/da..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:int>\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, sum(count) as destination_total\n",
    "FROM flight_data\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "ORDER BY sum(count) DESC\n",
    "LIMIT 5\n",
    "\"\"\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "about-consortium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "TakeOrderedAndProject(limit=5, orderBy=[destination_total#87L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#10,destination_total#87L])\n",
      "+- *(2) HashAggregate(keys=[DEST_COUNTRY_NAME#10], functions=[sum(cast(count#12 as bigint))])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#10, 200)\n",
      "      +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#10], functions=[partial_sum(cast(count#12 as bigint))])\n",
      "         +- *(1) FileScan csv [DEST_COUNTRY_NAME#10,count#12] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/opt/workspace/datain/flight-data/2015-summary.csv, file:/opt/workspace/da..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:int>\n"
     ]
    }
   ],
   "source": [
    "flightData.groupBy(\"DEST_COUNTRY_NAME\").sum(\"count\")\\\n",
    "    .withColumnRenamed(\"sum(count)\", \"destination_total\").sort(desc(\"destination_total\"))\\\n",
    "    .limit(5).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressive-budget",
   "metadata": {},
   "source": [
    "\n",
    "*Exactly the same execution path is used*, whether performing the operation via Spark SQL or on the data-frame API using pyspark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "oriented-tuner",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-number",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
