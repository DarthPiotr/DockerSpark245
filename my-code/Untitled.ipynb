{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20a4e348-2807-46b7-a744-af3ecf840ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/20 21:26:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Started distributed training with 2 executor processes\n",
      "[Stage 0:>                                                          (0 + 2) / 2][rank1]: Traceback (most recent call last):\n",
      "[rank1]:   File \"/tmp/tmpjkksynoi/train.py\", line 8, in <module>\n",
      "[rank1]:     output = train_fn(*args, **kwargs)\n",
      "[rank1]:              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/tmp/ipykernel_26/1546581349.py\", line 24, in train\n",
      "[rank1]: NameError: name 'createModel' is not defined\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/tmp/tmp1rw8hc4z/train.py\", line 8, in <module>\n",
      "[rank0]:     output = train_fn(*args, **kwargs)\n",
      "[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/tmp/ipykernel_26/1546581349.py\", line 24, in train\n",
      "[rank0]: NameError: name 'createModel' is not defined\n",
      "E1120 21:26:22.977000 139983966569088 torch/distributed/elastic/multiprocessing/api.py:826] failed (exitcode: 1) local_rank: 0 (pid: 294) of binary: /usr/local/bin/python3\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/usr/local/lib/python3.11/site-packages/torch/distributed/run.py\", line 883, in <module>\n",
      "E1120 21:26:22.977000 140106772640384 torch/distributed/elastic/multiprocessing/api.py:826] failed (exitcode: 1) local_rank: 0 (pid: 295) of binary: /usr/local/bin/python3\n",
      "Traceback (most recent call last):\n",
      "    main()\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"/usr/local/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/torch/distributed/run.py\", line 879, in main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "    run(args)\n",
      "  File \"/usr/local/lib/python3.11/site-packages/torch/distributed/run.py\", line 883, in <module>\n",
      "    main()\n",
      "  File \"/usr/local/lib/python3.11/site-packages/torch/distributed/run.py\", line 870, in run\n",
      "    elastic_launch(\n",
      "  File \"/usr/local/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 132, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 263, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "  File \"/usr/local/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/torch/distributed/run.py\", line 879, in main\n",
      "    run(args)\n",
      "  File \"/usr/local/lib/python3.11/site-packages/torch/distributed/run.py\", line 870, in run\n",
      "    elastic_launch(\n",
      "  File \"/usr/local/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 132, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 263, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError:\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError:\n",
      "============================================================\n",
      "/tmp/tmpjkksynoi/train.py FAILED\n",
      "============================================================\n",
      "/tmp/tmp1rw8hc4z/train.py FAILED\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "[0]:\n",
      "  time      : 2024-11-20_21:26:22\n",
      "  host      : dff8594aaf15\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 295)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "  time      : 2024-11-20_21:26:22\n",
      "  host      : 78c93e1b2d13\n",
      "============================================================\n",
      "  rank      : 1 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 294)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n",
      "24/11/20 21:26:23 WARN TaskSetManager: Lost task 1.0 in stage 0.0 (TID 0) (172.19.0.4 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/pyspark/ml/torch/distributor.py\", line 730, in wrapped_train_fn\n",
      "    LogStreamingClient._destroy()\n",
      "  File \"/usr/bin/spark-3.5.3-bin-hadoop3/python/lib/pyspark.zip/pyspark/ml/torch/distributor.py\", line 566, in _get_output_from_framework_wrapper\n",
      "    return framework_wrapper(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/bin/spark-3.5.3-bin-hadoop3/python/lib/pyspark.zip/pyspark/ml/torch/distributor.py\", line 907, in _run_training_on_pytorch_function\n",
      "    raise RuntimeError(\n",
      "RuntimeError: TorchDistributor failed during training.View stdout logs for detailed error message.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o53.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Could not recover from a failed barrier ResultStage. Most recent failure reason: Stage failed because barrier task ResultTask(0, 1) finished unsuccessfully.\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/site-packages/pyspark/ml/torch/distributor.py\", line 730, in wrapped_train_fn\n    LogStreamingClient._destroy()\n  File \"/usr/bin/spark-3.5.3-bin-hadoop3/python/lib/pyspark.zip/pyspark/ml/torch/distributor.py\", line 566, in _get_output_from_framework_wrapper\n    return framework_wrapper(\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/bin/spark-3.5.3-bin-hadoop3/python/lib/pyspark.zip/pyspark/ml/torch/distributor.py\", line 907, in _run_training_on_pytorch_function\n    raise RuntimeError(\nRuntimeError: TorchDistributor failed during training.View stdout logs for detailed error message.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:2228)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3054)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4149)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4146)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m     32\u001b[0m distributor \u001b[38;5;241m=\u001b[39m TorchDistributor(num_processes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, local_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, use_gpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 33\u001b[0m \u001b[43mdistributor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pyspark/ml/torch/distributor.py:1005\u001b[0m, in \u001b[0;36mTorchDistributor.run\u001b[0;34m(self, train_object, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_object: Union[Callable, \u001b[38;5;28mstr\u001b[39m], \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Any]:\n\u001b[1;32m    967\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Runs distributed training.\u001b[39;00m\n\u001b[1;32m    968\u001b[0m \n\u001b[1;32m    969\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;124;03m        a file.\u001b[39;00m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1005\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_object\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTorchDistributor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_training_on_pytorch_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pyspark/ml/torch/distributor.py:1025\u001b[0m, in \u001b[0;36mTorchDistributor._run\u001b[0;34m(self, train_object, run_pytorch_file_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1021\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_local_training(\n\u001b[1;32m   1022\u001b[0m         framework_wrapper_fn, train_object, run_pytorch_file_fn, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   1023\u001b[0m     )\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1025\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_distributed_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework_wrapper_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_object\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_pytorch_file_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pyspark/ml/torch/distributor.py:803\u001b[0m, in \u001b[0;36mTorchDistributor._run_distributed_training\u001b[0;34m(self, framework_wrapper_fn, train_object, run_pytorch_file_fn, spark_dataframe, *args, **kwargs)\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    798\u001b[0m     input_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspark\u001b[38;5;241m.\u001b[39mrange(\n\u001b[1;32m    799\u001b[0m         start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_tasks, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, numPartitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_tasks\n\u001b[1;32m    800\u001b[0m     )\n\u001b[1;32m    801\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[43minput_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapInArrow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspark_task_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchunk binary\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbarrier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m--> 803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m output_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([row\u001b[38;5;241m.\u001b[39mchunk \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m rows])\n\u001b[1;32m    805\u001b[0m result \u001b[38;5;241m=\u001b[39m cloudpickle\u001b[38;5;241m.\u001b[39mloads(output_bytes)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pyspark/sql/dataframe.py:1263\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m   1244\u001b[0m \n\u001b[1;32m   1245\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1260\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m-> 1263\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o53.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Could not recover from a failed barrier ResultStage. Most recent failure reason: Stage failed because barrier task ResultTask(0, 1) finished unsuccessfully.\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/site-packages/pyspark/ml/torch/distributor.py\", line 730, in wrapped_train_fn\n    LogStreamingClient._destroy()\n  File \"/usr/bin/spark-3.5.3-bin-hadoop3/python/lib/pyspark.zip/pyspark/ml/torch/distributor.py\", line 566, in _get_output_from_framework_wrapper\n    return framework_wrapper(\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/bin/spark-3.5.3-bin-hadoop3/python/lib/pyspark.zip/pyspark/ml/torch/distributor.py\", line 907, in _run_training_on_pytorch_function\n    raise RuntimeError(\nRuntimeError: TorchDistributor failed during training.View stdout logs for detailed error message.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:2228)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3054)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4149)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4146)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.torch.distributor import TorchDistributor\n",
    "from pyspark.sql import SparkSession\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn.parallel as p #.DistributedDataParallel as DDP\n",
    "from torch.utils.data import DistributedSampler, DataLoader\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-notebook-1\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        getOrCreate()\n",
    "\n",
    "def train(learning_rate, use_gpu):\n",
    "  import torch\n",
    "  import torch.distributed as dist\n",
    "  import torch.nn.parallel as p # import torch.nn.parallel.DistributedDataParallel as DDP\n",
    "  from torch.utils.data import DistributedSampler, DataLoader\n",
    "\n",
    "  backend = \"nccl\" if use_gpu else \"gloo\"\n",
    "  dist.init_process_group(backend)\n",
    "  device = int(os.environ[\"LOCAL_RANK\"]) if use_gpu  else \"cpu\"\n",
    "  model = p.DistributedDataParallel(createModel(), **kwargs)\n",
    "  sampler = DistributedSampler(dataset)\n",
    "  loader = DataLoader(dataset, sampler=sampler)\n",
    "\n",
    "  output = train(model, loader, learning_rate)\n",
    "  dist.cleanup()\n",
    "  return output\n",
    "\n",
    "distributor = TorchDistributor(num_processes=2, local_mode=False, use_gpu=False)\n",
    "distributor.run(train, 1e-3, False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5878038-f4aa-4a13-a53d-1e9d6e633a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark Version :3.5.3\n",
      "PySpark Version :3.5.3\n"
     ]
    }
   ],
   "source": [
    "print('PySpark Version :'+spark.version)\n",
    "print('PySpark Version :'+spark.sparkContext.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f58c92df-f31c-462d-b158-58e3f2b1b6c8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.9 (main, Nov 17 2024, 03:07:25) [GCC 10.2.1 20210110]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119f7f97-3cc6-4dbe-87b8-c76381004875",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Started distributed training with 2 executor processes\n",
      "[Stage 1:>                                                          (0 + 2) / 2][INFO] 17-Nov-24 22:12:02 - Running Mammoth! on 4685425b17e1. (if you see this message more than once, you are probably importing something wrong)\n",
      "[WARNING] 17-Nov-24 22:12:02 - Warning: python-dotenv not installed. Ignoring .env file.\n",
      "[INFO] 17-Nov-24 22:12:02 - Running Mammoth! on 4295d1da35c1. (if you see this message more than once, you are probably importing something wrong)\n",
      "[WARNING] 17-Nov-24 22:12:02 - Warning: python-dotenv not installed. Ignoring .env file.\n",
      "[WARNING] 17-Nov-24 22:12:03 - No backbone specified. Using default backbone (set by the dataset).\n",
      "[WARNING] 17-Nov-24 22:12:03 - No backbone specified. Using default backbone (set by the dataset).\n",
      "[WARNING] 17-Nov-24 22:12:04 - The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "[WARNING] 17-Nov-24 22:12:04 - The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "[WARNING] 17-Nov-24 22:12:04 - Error in model starprompt\n",
      "[WARNING] 17-Nov-24 22:12:04 - Error in model second_stage_starprompt\n",
      "[WARNING] 17-Nov-24 22:12:04 - Error in model starprompt\n",
      "[WARNING] 17-Nov-24 22:12:04 - Error in model second_stage_starprompt\n",
      "[WARNING] 17-Nov-24 22:12:05 - Trying to load default configuration for model der but no configuration file found in models/config/der.yaml.\n",
      "[WARNING] 17-Nov-24 22:12:05 - Default configuration file not found for dataset seq-mnist. Using the defaults specified in the dataset class (if available).\n",
      "[INFO] 17-Nov-24 22:12:05 - Using device cpu\n",
      "[INFO] 17-Nov-24 22:12:05 - `wandb_entity` and `wandb_project` not set. Disabling wandb.\n",
      "[INFO] 17-Nov-24 22:12:05 - Using backbone: mnistmlp\n",
      "Using BaseMNISTMLP as backbone\n",
      "[ERROR] 17-Nov-24 22:12:05 - could not initialize kornia transforms.\n",
      "Namespace(dataset='seq-mnist', model='der', backbone='mnistmlp', load_best_args=False, dataset_config=None, model_config='default', buffer_size=4096, minibatch_size=64, alpha=1.0, mlp_hidden_size=100, seed=None, permute_classes=0, base_path='./data/', results_path='results/', device=device(type='cpu'), notes=None, eval_epochs=None, non_verbose=0, disable_log=0, num_workers=None, enable_other_metrics=0, debug_mode=0, inference_only=0, code_optimization=0, distributed='no', savecheck=None, loadcheck=None, ckpt_name=None, start_from=None, stop_after=None, wandb_name=None, wandb_entity=None, wandb_project=None, lr=0.01, batch_size=64, label_perc=1, label_perc_by_class=1, joint=0, eval_future=0, validation=None, validation_mode='current', fitting_mode='epochs', early_stopping_patience=5, early_stopping_metric='loss', early_stopping_freq=1, early_stopping_epsilon=1e-06, n_epochs=1, n_iters=None, optimizer='sgd', optim_wd=0.0, optim_mom=0.0, optim_nesterov=0, lr_scheduler=None, scheduler_mode='epoch', lr_milestones=[], sched_multistep_lr_gamma=0.1, noise_type='symmetric', noise_rate=0, disable_noisy_labels_cache=0, cache_path_noisy_labels='noisy_labels', conf_jobnum='f5fe1851-d578-4012-827f-62f4a9409066', conf_timestamp='2024-11-17 22:12:05.010619', conf_host='4685425b17e1', conf_git_hash='3d6fc4b0645734e5d8c416293efadc44f3032382', nowand=1)\n",
      "\n",
      "[WARNING] 17-Nov-24 22:12:05 - Trying to load default configuration for model der but no configuration file found in models/config/der.yaml.\n",
      "[WARNING] 17-Nov-24 22:12:05 - Default configuration file not found for dataset seq-mnist. Using the defaults specified in the dataset class (if available).\n",
      "[INFO] 17-Nov-24 22:12:05 - Using device cpu\n",
      "[INFO] 17-Nov-24 22:12:05 - `wandb_entity` and `wandb_project` not set. Disabling wandb.\n",
      "[INFO] 17-Nov-24 22:12:05 - Using backbone: mnistmlp\n",
      "Using BaseMNISTMLP as backbone\n",
      "[ERROR] 17-Nov-24 22:12:05 - could not initialize kornia transforms.\n",
      "Namespace(dataset='seq-mnist', model='der', backbone='mnistmlp', load_best_args=False, dataset_config=None, model_config='default', buffer_size=4096, minibatch_size=64, alpha=1.0, mlp_hidden_size=100, seed=None, permute_classes=0, base_path='./data/', results_path='results/', device=device(type='cpu'), notes=None, eval_epochs=None, non_verbose=0, disable_log=0, num_workers=None, enable_other_metrics=0, debug_mode=0, inference_only=0, code_optimization=0, distributed='no', savecheck=None, loadcheck=None, ckpt_name=None, start_from=None, stop_after=None, wandb_name=None, wandb_entity=None, wandb_project=None, lr=0.01, batch_size=64, label_perc=1, label_perc_by_class=1, joint=0, eval_future=0, validation=None, validation_mode='current', fitting_mode='epochs', early_stopping_patience=5, early_stopping_metric='loss', early_stopping_freq=1, early_stopping_epsilon=1e-06, n_epochs=1, n_iters=None, optimizer='sgd', optim_wd=0.0, optim_mom=0.0, optim_nesterov=0, lr_scheduler=None, scheduler_mode='epoch', lr_milestones=[], sched_multistep_lr_gamma=0.1, noise_type='symmetric', noise_rate=0, disable_noisy_labels_cache=0, cache_path_noisy_labels='noisy_labels', conf_jobnum='8b77d12d-509d-44af-aaba-930692eaf45f', conf_timestamp='2024-11-17 22:12:05.011154', conf_host='4295d1da35c1', conf_git_hash='3d6fc4b0645734e5d8c416293efadc44f3032382', nowand=1)\n",
      "\n",
      "[INFO] 17-Nov-24 22:12:05 - Using 8 workers for the dataloader.\n",
      "[INFO] 17-Nov-24 22:12:05 - Using 8 workers for the dataloader.\n",
      "[INFO] 17-Nov-24 22:12:05 - Using 8 workers for the dataloader.\n",
      "[INFO] 17-Nov-24 22:12:05 - Using 8 workers for the dataloader.\n",
      "Task 1 - Epoch 1: 100%|██████████| 198/198 [00:00<00:00, 220.58it/s, loss=0.363, lr=0.01, ep/h=6.41e+3]\n",
      "[Stage 1:>                                                          (0 + 2) / 2]"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.torch.distributor import TorchDistributor\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-notebook-1\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        getOrCreate()\n",
    "\n",
    "distributor = TorchDistributor(\n",
    "    num_processes=2,\n",
    "    local_mode=False,\n",
    "    use_gpu=False)\n",
    "distributor.run(\"/opt/workspace/mammoth/utils/main.py\", \"--dataset\", \"seq-mnist\", \"--model\", \"der\", \"--buffer_size\", \"4096\", \"--alpha\", \"1\", \"--lr\", \"0.01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ed420c0-71d7-4f6f-8853-4b11775c1550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement cudatoolkit (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for cudatoolkit\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install cudatoolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b60c9d9-7ae1-41bb-9489-26cf8642d289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9743d7cd-109c-482f-aac1-999f33e80e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m torch.distributed.launch --nproc_per_node=4 --use_env example_1.py\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyTrainDataset(Dataset):\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.data = [(torch.rand(20), torch.rand(1)) for _ in range(size)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "def pprint(rank, msg):\n",
    "    # We add sleep to avoid printing clutter\n",
    "    time.sleep(1 * rank)\n",
    "    print(rank, msg)\n",
    "\n",
    "def main():\n",
    "    import torch\n",
    "    import torch.distributed as dist\n",
    "    \n",
    "    os.environ['WORLD_SIZE'] = \"2\"\n",
    "    # os.environ['MASTER_ADDR'] = '10.57.23.164'          \n",
    "    # os.environ['MASTER_PORT'] = '8888'  \n",
    "    dist.init_process_group(\"gloo\")\n",
    "    \n",
    "\n",
    "    rank = dist.get_rank()\n",
    "    pprint(rank, f\"world size = {dist.get_world_size()}\")            \n",
    "    pprint(rank, f\"backend = {dist.get_backend()}\")                   \n",
    "    pprint(rank,f\"rank = {dist.get_rank()}\")                          \n",
    "    a = 2\n",
    "    pprint(rank,f\"a = {a+dist.get_rank()}\")\n",
    "    dist.barrier()\n",
    "    pprint(rank,f\"first barrier broken by {rank}\")\n",
    "    b = 3\n",
    "    pprint(rank,f\"b = {b+dist.get_rank()}\")\n",
    "    dist.barrier()\n",
    "    pprint(rank,f\"second barrier broken by {rank}\")\n",
    "\n",
    "\n",
    "from pyspark.ml.torch.distributor import TorchDistributor\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-notebook-2\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        getOrCreate()\n",
    "distributor = TorchDistributor(num_processes=2, local_mode=False, use_gpu=False)\n",
    "distributor.run(main)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4d5df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "class ToyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ToyModel, self).__init__()\n",
    "        self.net1 = nn.Linear(10, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.net2 = nn.Linear(10, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net2(self.relu(self.net1(x)))\n",
    "\n",
    "\n",
    "def demo_basic(local_world_size, local_rank):\n",
    "\n",
    "    # setup devices for this process. For local_world_size = 2, num_gpus = 8,\n",
    "    # rank 0 uses GPUs [0, 1, 2, 3] and\n",
    "    # rank 1 uses GPUs [4, 5, 6, 7].\n",
    "    n = local_world_size\n",
    "    device_ids = list(range(local_rank * n, (local_rank + 1) * n))\n",
    "\n",
    "    print(\n",
    "        f\"[{os.getpid()}] rank = {dist.get_rank()}, \"\n",
    "        + f\"world_size = {dist.get_world_size()}, n = {n}, device_ids = {device_ids} \\n\", end=''\n",
    "    )\n",
    "\n",
    "    model = ToyModel().cuda(device_ids[0])\n",
    "    ddp_model = DDP(model, device_ids)\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = ddp_model(torch.randn(20, 10))\n",
    "    labels = torch.randn(20, 5).to(device_ids[0])\n",
    "    loss_fn(outputs, labels).backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "def spmd_main(local_world_size, local_rank):\n",
    "    # These are the parameters used to initialize the process group\n",
    "    env_dict = {\n",
    "        key: os.environ[key]\n",
    "        for key in (\"MASTER_ADDR\", \"MASTER_PORT\", \"RANK\", \"WORLD_SIZE\")\n",
    "    }\n",
    "    \n",
    "    if sys.platform == \"win32\":\n",
    "        # Distributed package only covers collective communications with Gloo\n",
    "        # backend and FileStore on Windows platform. Set init_method parameter\n",
    "        # in init_process_group to a local file.\n",
    "        if \"INIT_METHOD\" in os.environ.keys():\n",
    "            print(f\"init_method is {os.environ['INIT_METHOD']}\")\n",
    "            url_obj = urlparse(os.environ[\"INIT_METHOD\"])\n",
    "            if url_obj.scheme.lower() != \"file\":\n",
    "                raise ValueError(\"Windows only supports FileStore\")\n",
    "            else:\n",
    "                init_method = os.environ[\"INIT_METHOD\"]\n",
    "        else:\n",
    "            # It is a example application, For convience, we create a file in temp dir.\n",
    "            temp_dir = tempfile.gettempdir()\n",
    "            init_method = f\"file:///{os.path.join(temp_dir, 'ddp_example')}\"\n",
    "        dist.init_process_group(backend=\"nccl\", init_method=init_method, rank=int(env_dict[\"RANK\"]), world_size=int(env_dict[\"WORLD_SIZE\"]))\n",
    "    else:\n",
    "        print(f\"[{os.getpid()}] Initializing process group with: {env_dict}\")  \n",
    "        dist.init_process_group(backend=\"gloo\")\n",
    "\n",
    "    print(\n",
    "        f\"[{os.getpid()}]: world_size = {dist.get_world_size()}, \"\n",
    "        + f\"rank = {dist.get_rank()}, backend={dist.get_backend()} \\n\", end=''\n",
    "    )\n",
    "\n",
    "    demo_basic(local_world_size, local_rank)\n",
    "\n",
    "    # Tear down the process group\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     # This is passed in via launch.py\n",
    "#     parser.add_argument(\"--local_rank\", type=int, default=0)\n",
    "#     # This needs to be explicitly passed in\n",
    "#     parser.add_argument(\"--local_world_size\", type=int, default=1)\n",
    "#     args = parser.parse_args()\n",
    "#     # The main entry point is called directly without using subprocess\n",
    "#\n",
    "# spmd_main(args.local_world_size, args.local_rank)\n",
    "\n",
    "from pyspark.ml.torch.distributor import TorchDistributor\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-notebook-2\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        getOrCreate()\n",
    "distributor = TorchDistributor(num_processes=2, local_mode=False, use_gpu=False)\n",
    "distributor.run(spmd_main, 1, 1)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aac7684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m torch.distributed.launch --nproc_per_node=4 --use_env example_1.py\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyTrainDataset(Dataset):\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.data = [(torch.rand(20), torch.rand(1)) for _ in range(size)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "def pprint(rank, msg):\n",
    "    # We add sleep to avoid printing clutter\n",
    "    time.sleep(1 * rank)\n",
    "    print(rank, msg)\n",
    "\n",
    "def main():\n",
    "    import torch\n",
    "    import torch.distributed as dist\n",
    "    # import torch.nn.parallel.DistributedDataParallel as DDP\n",
    "    # from torch.utils.data import DistributedSampler, DataLoader\n",
    "    \n",
    "    os.environ['WORLD_SIZE'] = \"2\"\n",
    "    # os.environ['MASTER_ADDR'] = '10.57.23.164'          \n",
    "    # os.environ['MASTER_PORT'] = '8888'  \n",
    "    dist.init_process_group(\"gloo\")\n",
    "\n",
    "    \n",
    "    # train_set = MyTrainDataset(2048)  # load your dataset\n",
    "    # model = DPP(torch.nn.Linear(20, 1), **kwargs)  # load your model\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "    # sampler = DistributedSampler(dataset)\n",
    "    # loader = DataLoader(dataset, sampler=sampler)\n",
    "    \n",
    "\n",
    "    rank = dist.get_rank()\n",
    "    pprint(rank, f\"world size = {dist.get_world_size()}\")            \n",
    "    pprint(rank, f\"backend = {dist.get_backend()}\")                   \n",
    "    pprint(rank,f\"rank = {dist.get_rank()}\")                          \n",
    "    a = 2\n",
    "    pprint(rank,f\"a = {a+dist.get_rank()}\")\n",
    "    dist.barrier()\n",
    "    pprint(rank,f\"first barrier broken by {rank}\")\n",
    "    b = 3\n",
    "    pprint(rank,f\"b = {b+dist.get_rank()}\")\n",
    "    dist.barrier()\n",
    "    pprint(rank,f\"second barrier broken by {rank}\")\n",
    "\n",
    "\n",
    "from pyspark.ml.torch.distributor import TorchDistributor\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-notebook-2\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        getOrCreate()\n",
    "distributor = TorchDistributor(num_processes=2, local_mode=False, use_gpu=False)\n",
    "distributor.run(main)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbefa995",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
